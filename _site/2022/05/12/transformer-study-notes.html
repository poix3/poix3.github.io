<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformer Study Notes | Life in the Woods</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Transformer Study Notes" />
<meta name="author" content="Fengmao Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="“Attention is all you need.”" />
<meta property="og:description" content="“Attention is all you need.”" />
<link rel="canonical" href="http://localhost:4000/2022/05/12/transformer-study-notes.html" />
<meta property="og:url" content="http://localhost:4000/2022/05/12/transformer-study-notes.html" />
<meta property="og:site_name" content="Life in the Woods" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-12T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transformer Study Notes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Fengmao Wang"},"dateModified":"2022-05-12T00:00:00+02:00","datePublished":"2022-05-12T00:00:00+02:00","description":"“Attention is all you need.”","headline":"Transformer Study Notes","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/05/12/transformer-study-notes.html"},"url":"http://localhost:4000/2022/05/12/transformer-study-notes.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Life in the Woods" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Life in the Woods</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/CV/">CV</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformer Study Notes</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-05-12T00:00:00+02:00" itemprop="datePublished">
        May 12, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>“Attention is all you need.”</p>

<ul>
  <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>.</li>
  <li><a href="https://www.bilibili.com/video/BV1pu411o7BE">Transformer论文逐段精读</a></li>
  <li><a href="https://www.youtube.com/watch?v=ugWDIIOHtPA&amp;t=2401s">Transformer, Hung-yi Lee</a></li>
</ul>

<h3 id="1-introduction">1. Introduction</h3>

<p>Recurrent models：</p>

<ul>
  <li>factor computation along the symbol positions</li>
  <li>align the positions to steps in computation time ($h_{t-1}\rightarrow h_t$)</li>
  <li>preclude parallelization within training examples (sequential computation)</li>
</ul>

<p>Attention mechanism：</p>

<ul>
  <li>allow modeling of dependencies without regard to their distance in the input or output sequences</li>
</ul>

<h3 id="2-background">2. Background</h3>

<p>ConvS2S and ByteNet are difficult to learn dependencies between distant positions and the number of operations grows required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly and logarithmically respectively.</p>

<p>CNN代替RNN、但使用卷积很难对长的序列进行建模：很多层卷积才能将隔得比较远的位置关联起来。而卷积能一层就把整个序列”看到“。</p>

<p>In the Transformer this is reduced to a constant number of operations. Multi-head attention is used to simulate the multi-channel output of CNN.</p>

<p>但卷积能输出多个通道（不同的特征、模式etc.），所以用多头注意力来模拟。</p>

<h3 id="3-model-architecture">3. Model Architecture</h3>

<p><img src="/pic/transformer.png" style="zoom:50%;" /></p>

<p>Encoder maps an input sequence of symbol representations (x1,…,xn) to a sequence of continuous representations z = (z1,…,zn)</p>

<ul>
  <li>where $z_t$ is the vector form of $x_t$</li>
</ul>

<p>Decoder then generates an output sequence (y1, …, ym)</p>

<ul>
  <li>auto-regressive: 解码器中词是一个一个生成的，且“输入又是输出”
    <ul>
      <li>prevent positions from attending to subsequent positions</li>
      <li>因为在注意力机制中能看到<strong>整个</strong>句子，为了避免t时间能看到t时间以后的输入， 所以采用了masked多头注意力</li>
      <li>保证训练和预测的行为一致</li>
    </ul>
  </li>
  <li><strong>NB:</strong> m!=n</li>
</ul>

<p>Layer normalization 对每一个<strong>样本</strong>做normalization，</p>

<ul>
  <li>全局的均值方差对不定长的句子（比如比训练句更长的测试句）来讲效果不是特别好，所以不用BN</li>
</ul>

<p><img src="/pic/layer_normalization.png" style="zoom:35%;" /></p>

<p>Scaled Dot-Product Attention</p>

<ul>
  <li>“scaled”: <a href="https://blog.csdn.net/qq_37430422/article/details/105042303">为什么 dot-product attention 需要被 scaled？</a> “We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients”</li>
  <li>矩阵乘法利好并行</li>
</ul>


  </div><a class="u-url" href="/2022/05/12/transformer-study-notes.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">Fengmao Wang</li>
          <li><a class="u-email" href="mailto:wangfm2020@gmail.com">wangfm2020@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>“So we beat on, boats against the current, borne back ceaselessly into the past.”
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/poix3" target="_blank" title="poix3"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/wangfm" target="_blank" title="wangfm"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
